# Что такое **токены**?

ИИ не «читает» текст так, как человек.  
Он **разбивает текст на мелкие единицы — токены**, и именно с ними работает.

**Что такое токен?**
**Токен** — это **кусочек текста**, который может быть:
- целым словом (например: `как`)
- частью слова (`информа` + `ция`)
- знаком препинания (`?`, `,`)
- спец. символом (`\n`, `@`, `#`)
Один токен ≠ одно слово. Это может быть **полуслово**, **корень**, или даже **одна буква**.

**Пример на русском:**
Фраза:  
**«Привет, как дела?»**

Может быть разбита на такие токены:
1. `Привет`
2. `,`
3. `как`
4. `дела`
5. `?`  
**Итого: 5 токенов**

**Пример на английском:**
Фраза:  
**"I'm learning prompting."**
Разбивается на:
1. `I`
2. `'m`
3. `learning`
4. `prompt`
5. `ing`
6. `.`  
**Итого: 6 токенов**

Некоторые слова (особенно длинные или составные) разбиваются на **несколько токенов**.


#  Почему модель работает с токенами?

Потому что:
- **Цифровой текст** легче разбивать на блоки фиксированной длины
- Один токен можно закодировать числом → передать в модель
- Это делает модель **универсальной** для всех языков: английский, русский, японский — всё работает с токенами

 **Пример токенизации:**
`Слово: "Привет"   Токен ID: 11235`  
ИИ работает не с "Привет", а с числом `11235`.  
Модель не «понимает слово» — она **анализирует числа**, соответствующие токенам.

Итого:

|Что|Объяснение|
|---|---|
|Токен|Единица текста: часть слова, слово, символ|
|Модель|Работает с числами, а не словами|
|Польза|Универсальность, скорость, компактность|
|Влияние|На длину промта и ограничение диалога|



# Ограничение на количество токенов
Каждая ИИ-модель работает в **рамках максимального лимита токенов**.  
Примеры:

|Модель|Максимум токенов|
|---|---|
|GPT-3.5-turbo|~4 096 токенов|
|GPT-4 (стандарт)|~8 192 токена|
|GPT-4-32k|~32 768 токенов|
**Что входит в лимит?**
**Всё**:
- твой запрос (включая весь текст промта)
- весь предыдущий диалог
- ответ модели
То есть:  
**чем больше промт → тем меньше места остаётся на ответ.**

**Что происходит при переполнении?**
Если токены заканчиваются:
- модель **может обрезать ответ** на полуслове
- модель **«забывает» начало диалога** (в GPT-3.5 это видно особенно сильно)
- ты получаешь **неполную информацию**

**Пример:**
Ты отправил промт длиной на 3 000 токенов.  
Если лимит — 4 000, то на **ответ остаётся только 1 000 токенов**.  
Если ИИ не успевает — он просто **обрывается**.

Это не баг. Это предел памяти.

**Миф:** «Если ИИ перестал отвечать — он завис.»
**На самом деле:**  
чаще всего — **просто закончились токены.**  
Модель не может продолжать, потому что лимит достигнут.

# Как работают токены ?

|Этап|Что делает модель|
|---|---|
|Текст → токены|Разбивает на единицы|
|Токены → векторы|Представляет как числа|
|Векторы → предсказания|Угадывает следующий токен на основе всех предыдущих|

